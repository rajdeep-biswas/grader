{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grading.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm2Tp48NJZhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnzZmUSbZRWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "acbcdd7c-a7a4-47f5-9564-0f088ec6fdfd"
      },
      "source": [
        "!pip install https://github.com/MartinoMensio/spacy-universal-sentence-encoder-tfhub/releases/download/en_use_lg-0.2.0/en_use_lg-0.2.0.tar.gz#en_use_lg-0.2.0"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/MartinoMensio/spacy-universal-sentence-encoder-tfhub/releases/download/en_use_lg-0.2.0/en_use_lg-0.2.0.tar.gz#en_use_lg-0.2.0\n",
            "\u001b[?25l  Downloading https://github.com/MartinoMensio/spacy-universal-sentence-encoder-tfhub/releases/download/en_use_lg-0.2.0/en_use_lg-0.2.0.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 335kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from en-use-lg==0.2.0) (2.2.4)\n",
            "Collecting universal_sentence_encoder@ git+ssh://git@github.com/MartinoMensio/spacy-universal-sentence-embedding-tfhub\n",
            "  Cloning ssh://****@github.com/MartinoMensio/spacy-universal-sentence-embedding-tfhub to /tmp/pip-install-vwbzk71p/universal-sentence-encoder\n",
            "  Running command git clone -q 'ssh://****@github.com/MartinoMensio/spacy-universal-sentence-embedding-tfhub' /tmp/pip-install-vwbzk71p/universal-sentence-encoder\n",
            "  Host key verification failed.\n",
            "  fatal: Could not read from remote repository.\n",
            "\n",
            "  Please make sure you have the correct access rights\n",
            "  and the repository exists.\n",
            "\u001b[31mERROR: Command errored out with exit status 128: git clone -q 'ssh://****@github.com/MartinoMensio/spacy-universal-sentence-embedding-tfhub' /tmp/pip-install-vwbzk71p/universal-sentence-encoder Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHsC4NmgJ11t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KajHLROJ2Dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXLzCoO9LVjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvwbBZUuLtr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if token.text in nlp.Defaults.stop_words:\n",
        "            continue\n",
        "        if token.is_punct:\n",
        "            continue\n",
        "        if token.pos_ in ['PRON', 'VERB']:\n",
        "            continue\n",
        "        result.append(token.lemma_)\n",
        "    return \" \".join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRcm9Oe0LoYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers = [\"Heidi lived in Germany with her grandfather.\", \"Heidi lived in Malbazar.\", \"Heidi lived in Germany with her mother.\", \"Heidi lived in Ireland.\", \"Heidi and her grandfather lived in Germany.\"]\n",
        "parsedanswers = []\n",
        "\n",
        "for answer in answers:\n",
        "  parsedanswers.append(process_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCZZ-k8PQei0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answerkey = parsedanswers[0]\n",
        "attempts = parsedanswers[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_GyBn3yUIkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkpropns(answer, answerkey):\n",
        "  for token in nlp(answer):\n",
        "    print(token, token.pos_)\n",
        "    if token.pos_ == 'PROPN' and str(token) in answerkey:\n",
        "      1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnuIPpikSIxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "a1a1e82d-14f2-47b1-fc14-0a389a99808a"
      },
      "source": [
        "for answer in attempts:\n",
        "  print(nlp(answerkey).similarity(nlp(answer)), answer, \" | \", answerkey)\n",
        "  checkpropns(answer, answerkey)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.807541410339882 heidi live malbazar  |  heidi live germany grandfather\n",
            "heidi ADJ\n",
            "live ADJ\n",
            "malbazar NOUN\n",
            "0.9519856436708373 heidi live germany mother  |  heidi live germany grandfather\n",
            "heidi PROPN\n",
            "live PROPN\n",
            "germany PROPN\n",
            "mother NOUN\n",
            "0.8135614547524771 heidi live ireland  |  heidi live germany grandfather\n",
            "heidi PROPN\n",
            "live PROPN\n",
            "ireland PROPN\n",
            "0.9546968031717056 heidi grandfather live germany  |  heidi live germany grandfather\n",
            "heidi PROPN\n",
            "grandfather PROPN\n",
            "live VERB\n",
            "germany PROPN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doy31hYcSSYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUHRE4cSbPWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers = [\"Heidi lived in Germany with her grandfather.\", \"Heidi lived in Malbazar.\", \"Heidi lived in Germany with her mother.\", \"Heidi lived in Ireland.\", \"Heidi and her grandfather lived in Germany.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uU-M-IkdLb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsedanswers = []\n",
        "\n",
        "for answer in answers:\n",
        "  parsedanswers.append(process_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT3jqorbbVNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34cdb888-c976-45e0-95ef-c5c6a06835a1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(parsedanswers)\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg51g44JbSrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answerkey = parsedanswers[0]\n",
        "attempts = parsedanswers[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfsf8zuKbjJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "86600668-d5e6-4298-c669-147642d93163"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.36027901, 0.32705856, 0.44846234, 0.22905291,\n",
              "        0.1701679 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tV8wu00bxXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers = [\"In 1953 , they decided to create a desk job for Hooper because he was paralysed and couldn't work as a sales manager since it involved a lot of travel.\",\n",
        "  \"In 1969, a decision to assign Hooper a desk job was made as he was physically invalid. As a result, he couldn't work as a sales manager which demanded him to travel a lot.\",\n",
        "  \"Hooper was given a desk job in 1953 because he became paralyzed and couldn't travel extensively.\",\n",
        "  \"Hooper couldn't work as a sales manager because of his paralysis and hence they decided to create a desk job for him.\",\n",
        "  \"They decided to assign a desk job to Hooper because he was inefficient.\",\n",
        "  \"Since Hooper became physically invalid and had to avoid travelling, in 1992, they created a desk job for him.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4-nGMPZeORJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "19a6447c-d17d-4050-c6fc-82d56223f536"
      },
      "source": [
        "parsedanswers"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1953 desk job hooper paralysed sale manager lot travel',\n",
              " '1969 decision desk job physically invalid result sale manager lot',\n",
              " 'hooper desk job 1953 paralyzed extensively',\n",
              " 'hooper sale manager paralysis desk job',\n",
              " 'desk job hooper inefficient',\n",
              " 'hooper physically invalid 1992 desk job']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcHbEUw9hpUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}